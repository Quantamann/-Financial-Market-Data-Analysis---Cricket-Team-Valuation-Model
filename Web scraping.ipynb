{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bde5e678-b984-4340-b0ba-d797870f2c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed! Data saved to cricket_stats.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "import time\n",
    "\n",
    "class CricketScraper:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://www.espncricinfo.com\"\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2.1 Safari/605.1.15'\n",
    "        }\n",
    "\n",
    "    def get_match_links(self, tournament_id: str) -> List[str]:\n",
    "        \"\"\"Collect all match summary links from the tournament page\"\"\"\n",
    "        url = f'https://stats.espncricinfo.com/ci/engine/records/team/match_results.html?id=14450;type=tournament'\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find all rows in the match results table\n",
    "            match_rows = soup.select(table.ds-w-full ds-table ds-table-xs ds-table-auto ds-w-full ds-overflow-scroll ds-scrollbar-hide > tbody > tr.ds-bg-fill-content-alternate ds-text-left')\n",
    "            \n",
    "            # Extract match links\n",
    "            match_links = []\n",
    "            for row in match_rows:\n",
    "                link = row.select_one('td:nth-child(7) a')\n",
    "                if link and link.get('href'):\n",
    "                    full_url = self.base_url + link['href']\n",
    "                    match_links.append(full_url)\n",
    "            \n",
    "            return match_links\n",
    "        \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching match links: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_match_details(self, match_url: str) -> List[Dict]:\n",
    "        \"\"\"Extract batting statistics from a single match page\"\"\"\n",
    "        try:\n",
    "            response = requests.get(match_url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find match details\n",
    "            match_details = soup.find_all('div', string=\"Match Details\")\n",
    "            if not match_details:\n",
    "                return []\n",
    "            \n",
    "            # Get team names\n",
    "            innings_headers = soup.select('div.ds-text-tight-m')\n",
    "            team1 = innings_headers[0].get_text().replace(' Innings', '').strip()\n",
    "            team2 = innings_headers[1].get_text().replace(' Innings', '').strip()\n",
    "            match_info = f'{team1} Vs {team2}'\n",
    "            \n",
    "            # Get scorecard tables\n",
    "            tables = soup.select('table.ci-scorecard-table')\n",
    "            if len(tables) < 2:\n",
    "                return []\n",
    "            \n",
    "            batting_summary = []\n",
    "            \n",
    "            # Process both innings\n",
    "            for inning, table in enumerate(tables[:2]):\n",
    "                team = team1 if inning == 0 else team2\n",
    "                rows = table.select('tbody > tr')\n",
    "                \n",
    "                batting_pos = 1\n",
    "                for row in rows:\n",
    "                    # Check if it's a batting row (has 8 or more columns)\n",
    "                    cells = row.select('td')\n",
    "                    if len(cells) >= 8:\n",
    "                        try:\n",
    "                            batting_summary.append({\n",
    "                                'match': match_info,\n",
    "                                'teamInnings': team,\n",
    "                                'battingPos': batting_pos,\n",
    "                                'batsmanName': cells[0].select_one('a span').get_text().replace(' ', ''),\n",
    "                                'dismissal': cells[1].select_one('span').get_text().strip(),\n",
    "                                'runs': cells[2].select_one('strong').get_text(),\n",
    "                                'balls': cells[3].get_text(),\n",
    "                                '4s': cells[5].get_text(),\n",
    "                                '6s': cells[6].get_text(),\n",
    "                                'SR': cells[7].get_text()\n",
    "                            })\n",
    "                            batting_pos += 1\n",
    "                        except (AttributeError, IndexError) as e:\n",
    "                            print(f\"Error processing row: {e}\")\n",
    "                            continue\n",
    "            \n",
    "            return batting_summary\n",
    "        \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error processing match {match_url}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def scrape_tournament(self, tournament_id: str) -> pd.DataFrame:\n",
    "        \"\"\"Scrape entire tournament and return data as DataFrame\"\"\"\n",
    "        match_links = self.get_match_links(tournament_id)\n",
    "        all_batting_data = []\n",
    "        \n",
    "        for link in match_links:\n",
    "            print(f\"Processing match: {link}\")\n",
    "            match_data = self.get_match_details(link)\n",
    "            all_batting_data.extend(match_data)\n",
    "            # Be nice to the server\n",
    "            time.sleep(1)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        return pd.DataFrame(all_batting_data)\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = CricketScraper()\n",
    "    # Example tournament ID\n",
    "    tournament_id = \"14450\"\n",
    "    \n",
    "    # Scrape the tournament\n",
    "    df = scraper.scrape_tournament(tournament_id)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv('cricket_stats.csv', index=False)\n",
    "    print(\"Scraping completed! Data saved to cricket_stats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "601fd9f7-f50d-4531-bd28-fee073e3bd9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05fb4187-e0d3-4e48-9756-73bd3e48fbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_match_links():\n",
    "    url = \"https://stats.espncricinfo.com/ci/engine/records/team/match_results.html?id=14450;type=tournament\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    links = []\n",
    "    rows = soup.select('table.engineTable > tbody > tr.data1')\n",
    "    for row in rows:\n",
    "        link = \"https://www.espncricinfo.com\" + row.select_one('td:nth-child(7) a')['href']\n",
    "        links.append(link)\n",
    "    return links\n",
    "\n",
    "def parse_match_data(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Get match details\n",
    "    match_div = soup.find('div', string=lambda text: text and \"Match Details\" in text).parent.parent.parent\n",
    "    innings_divs = match_div.find_next_siblings('div')[:2]\n",
    "    team1 = innings_divs[0].find('span').text.replace(\" Innings\", \"\")\n",
    "    team2 = innings_divs[1].find('span').text.replace(\" Innings\", \"\")\n",
    "    match_info = f\"{team1} Vs {team2}\"\n",
    "    \n",
    "    batting_summary = []\n",
    "    tables = soup.select('div > table.ci-scorecard-table')\n",
    "    \n",
    "    # Process both innings\n",
    "    for inning, team in enumerate([team1, team2]):\n",
    "        rows = tables[inning].select('tbody > tr')\n",
    "        batting_pos = 1\n",
    "        \n",
    "        for row in rows:\n",
    "            cols = row.select('td')\n",
    "            if len(cols) >= 8:\n",
    "                batting_summary.append({\n",
    "                    \"match\": match_info,\n",
    "                    \"teamInnings\": team,\n",
    "                    \"battingPos\": batting_pos,\n",
    "                    \"batsmanName\": cols[0].select_one('a > span > span').text.replace(' ', ''),\n",
    "                    \"dismissal\": cols[1].select_one('span > span').text,\n",
    "                    \"runs\": cols[2].select_one('strong').text,\n",
    "                    \"balls\": cols[3].text,\n",
    "                    \"4s\": cols[5].text,\n",
    "                    \"6s\": cols[6].text,\n",
    "                    \"SR\": cols[7].text\n",
    "                })\n",
    "                batting_pos += 1\n",
    "    \n",
    "    return batting_summary\n",
    "\n",
    "def main():\n",
    "    all_batting_data = []\n",
    "    match_links = get_match_links()\n",
    "    \n",
    "    for link in match_links:\n",
    "        try:\n",
    "            match_data = parse_match_data(link)\n",
    "            all_batting_data.extend(match_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {link}: {str(e)}\")\n",
    "    \n",
    "    # Convert to DataFrame and save\n",
    "    df = pd.DataFrame(all_batting_data)\n",
    "    df.to_csv('cricket_batting_stats.csv', index=False)\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c79dd26f-ded6-4c36-9762-b20694ad48a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to cricket_stats_20241226_232250.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "class CricketScraper:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://www.espncricinfo.com\"\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2.1 Safari/605.1.15'\n",
    "        }\n",
    "\n",
    "    def get_match_links(self, tournament_id: str) -> List[str]:\n",
    "        url = f'https://stats.espncricinfo.com/ci/engine/records/team/match_results.html?id={tournament_id};type=tournament'\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            match_rows = soup.select('table.engineTable > tbody > tr.data1')\n",
    "            match_links = []\n",
    "            for row in match_rows:\n",
    "                link = row.select_one('td:nth-child(7) a')\n",
    "                if link and link.get('href'):\n",
    "                    full_url = self.base_url + link['href']\n",
    "                    match_links.append(full_url)\n",
    "            \n",
    "            return match_links\n",
    "        \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching match links: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_match_details(self, match_url: str) -> List[Dict]:\n",
    "        try:\n",
    "            response = requests.get(match_url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            match_details = soup.find_all('div', string=\"Match Details\")\n",
    "            if not match_details:\n",
    "                return []\n",
    "            \n",
    "            innings_headers = soup.select('div.ds-text-tight-m')\n",
    "            team1 = innings_headers[0].get_text().replace(' Innings', '').strip()\n",
    "            team2 = innings_headers[1].get_text().replace(' Innings', '').strip()\n",
    "            match_info = f'{team1} Vs {team2}'\n",
    "            \n",
    "            tables = soup.select('table.ci-scorecard-table')\n",
    "            if len(tables) < 2:\n",
    "                return []\n",
    "            \n",
    "            batting_summary = []\n",
    "            \n",
    "            for inning, table in enumerate(tables[:2]):\n",
    "                team = team1 if inning == 0 else team2\n",
    "                rows = table.select('tbody > tr')\n",
    "                \n",
    "                batting_pos = 1\n",
    "                for row in rows:\n",
    "                    cells = row.select('td')\n",
    "                    if len(cells) >= 8:\n",
    "                        try:\n",
    "                            batting_summary.append({\n",
    "                                'match': match_info,\n",
    "                                'teamInnings': team,\n",
    "                                'battingPos': batting_pos,\n",
    "                                'batsmanName': cells[0].select_one('a span').get_text().replace(' ', ''),\n",
    "                                'dismissal': cells[1].select_one('span').get_text().strip(),\n",
    "                                'runs': cells[2].select_one('strong').get_text(),\n",
    "                                'balls': cells[3].get_text(),\n",
    "                                '4s': cells[5].get_text(),\n",
    "                                '6s': cells[6].get_text(),\n",
    "                                'SR': cells[7].get_text(),\n",
    "                                'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                                'source_url': match_url\n",
    "                            })\n",
    "                            batting_pos += 1\n",
    "                        except (AttributeError, IndexError) as e:\n",
    "                            print(f\"Error processing row: {e}\")\n",
    "                            continue\n",
    "            \n",
    "            return batting_summary\n",
    "        \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error processing match {match_url}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def scrape_tournament(self, tournament_id: str) -> pd.DataFrame:\n",
    "        match_links = self.get_match_links(tournament_id)\n",
    "        all_batting_data = []\n",
    "        \n",
    "        for link in match_links:\n",
    "            print(f\"Processing match: {link}\")\n",
    "            match_data = self.get_match_details(link)\n",
    "            all_batting_data.extend(match_data)\n",
    "            time.sleep(1)\n",
    "        \n",
    "        return pd.DataFrame(all_batting_data)\n",
    "\n",
    "def save_cricket_data():\n",
    "    scraper = CricketScraper()\n",
    "    tournament_id = \"14450\"\n",
    "    df = scraper.scrape_tournament(tournament_id)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"cricket_stats_{timestamp}.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    save_cricket_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1765cc25-a07b-4578-bf89-d70fa0b21067",
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Read the CSV file\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcricket_stats_20241226_232250.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Display the first few rows and basic information\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame Shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, df\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1723\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1720\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[1;32m   1724\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1725\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m parsers\u001b[38;5;241m.\u001b[39mTextReader(src, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32mparsers.pyx:586\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('cricket_stats_20241226_232250.csv')\n",
    "\n",
    "# Display the first few rows and basic information\n",
    "print(\"DataFrame Shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nColumns:\", list(df.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79245a38-ee2f-4c54-8fa7-e5ac38ff08a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to cricket_stats_20241226_232711.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def scrape_cricket_data():\n",
    "    url = \"https://stats.espncricinfo.com/ci/engine/records/team/match_results.html?id=14450;type=tournament\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    match_rows = soup.select('table.engineTable > tbody > tr.data1')\n",
    "    all_batting_data = []\n",
    "    \n",
    "    for row in match_rows:\n",
    "        match_url = \"https://www.espncricinfo.com\" + row.select_one('td:nth-child(7) a')['href']\n",
    "        print(f\"Processing: {match_url}\")\n",
    "        \n",
    "        match_response = requests.get(match_url, headers=headers)\n",
    "        match_soup = BeautifulSoup(match_response.content, 'html.parser')\n",
    "        \n",
    "        innings = match_soup.select('div.ds-text-tight-m')\n",
    "        if len(innings) < 2:\n",
    "            continue\n",
    "            \n",
    "        team1 = innings[0].text.replace(' Innings', '').strip()\n",
    "        team2 = innings[1].text.replace(' Innings', '').strip()\n",
    "        match_info = f'{team1} Vs {team2}'\n",
    "        \n",
    "        tables = match_soup.select('table.ci-scorecard-table')\n",
    "        for inning, table in enumerate(tables[:2]):\n",
    "            team = team1 if inning == 0 else team2\n",
    "            rows = table.select('tbody > tr')\n",
    "            \n",
    "            batting_pos = 1\n",
    "            for player_row in rows:\n",
    "                cells = player_row.select('td')\n",
    "                if len(cells) >= 8:\n",
    "                    all_batting_data.append({\n",
    "                        'match': match_info,\n",
    "                        'teamInnings': team,\n",
    "                        'battingPos': batting_pos,\n",
    "                        'batsmanName': cells[0].select_one('a > span').text.strip(),\n",
    "                        'dismissal': cells[1].select_one('span').text.strip(),\n",
    "                        'runs': cells[2].select_one('strong').text.strip(),\n",
    "                        'balls': cells[3].text.strip(),\n",
    "                        '4s': cells[5].text.strip(),\n",
    "                        '6s': cells[6].text.strip(),\n",
    "                        'SR': cells[7].text.strip()\n",
    "                    })\n",
    "                    batting_pos += 1\n",
    "        time.sleep(1)\n",
    "    \n",
    "    df = pd.DataFrame(all_batting_data)\n",
    "    filename = f\"cricket_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Data saved to {filename}\")\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = scrape_cricket_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6758570-bde6-4614-b4df-1bcd822fceff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching the webpage: 403 Client Error: Forbidden for url: https://stats.espncricinfo.com/ci/engine/records/team/match_results.html?id=14450;type=tournament\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_cricket_stats():\n",
    "    # URL for the cricket statistics\n",
    "    url = 'https://stats.espncricinfo.com/ci/engine/records/team/match_results.html?id=14450;type=tournament'\n",
    "    \n",
    "    # Send HTTP request and get the page content\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching the webpage: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the target table\n",
    "    table = soup.find('table', class_='engineTable')\n",
    "    if not table:\n",
    "        print(\"Could not find the target table\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize list to store match data\n",
    "    match_summary = []\n",
    "    \n",
    "    # Find all data rows (excluding header)\n",
    "    rows = table.find_all('tr', class_='data1')\n",
    "    \n",
    "    # Extract data from each row\n",
    "    for row in rows:\n",
    "        cells = row.find_all('td')\n",
    "        if len(cells) >= 7:  # Ensure we have all required cells\n",
    "            match_data = {\n",
    "                'team1': cells[0].get_text(strip=True),\n",
    "                'team2': cells[1].get_text(strip=True),\n",
    "                'winner': cells[2].get_text(strip=True),\n",
    "                'margin': cells[3].get_text(strip=True),\n",
    "                'ground': cells[4].get_text(strip=True),\n",
    "                'matchDate': cells[5].get_text(strip=True),\n",
    "                'scorecard': cells[6].get_text(strip=True)\n",
    "            }\n",
    "            match_summary.append(match_data)\n",
    "    \n",
    "    return {\n",
    "        \"matchSummary\": match_summary\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # Scrape the data\n",
    "    result = scrape_cricket_stats()\n",
    "    \n",
    "    if result:\n",
    "        # Convert to DataFrame for easier viewing/manipulation\n",
    "        df = pd.DataFrame(result['matchSummary'])\n",
    "        print(\"Successfully scraped match data:\")\n",
    "        print(df)\n",
    "        \n",
    "        # Optionally save to CSV\n",
    "        df.to_csv('cricket_matches.csv', index=False)\n",
    "        print(\"\\nData has been saved to 'cricket_matches.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91491b11-41f5-474f-bd04-e037f862a747",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2838398622.py, line 37)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[20], line 37\u001b[0;36m\u001b[0m\n\u001b[0;31m    rows = table.find_all('tr', class_=$0)\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "class CricketScraper:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://www.espncricinfo.com\"\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2.1 Safari/605.1.15'\n",
    "        }\n",
    "    \n",
    "    def scrape_cricket_stats(self, tournament_id=\"14450\"):\n",
    "        # Construct the URL for the cricket statistics\n",
    "        url = f'https://stats.espncricinfo.com/ci/engine/records/team/match_results.html?id={tournament_id};type=tournament'\n",
    "        \n",
    "        # Send HTTP request and get the page content\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()  # Raise an exception for bad status codes\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching the webpage: {e}\")\n",
    "            return None\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find the target table\n",
    "        table = soup.find('table', class_='ds-w-full ds-table ds-table-xs ds-table-auto ds-w-full ds-overflow-scroll ds-scrollbar-hide')\n",
    "        if not table:\n",
    "            print(\"Could not find the target table\")\n",
    "            return None\n",
    "        \n",
    "        # Initialize list to store match data\n",
    "        match_summary = []\n",
    "        \n",
    "        # Find all data rows (excluding header)\n",
    "        rows = table.find_all('tr', class_='$0')\n",
    "        \n",
    "        # Extract data from each row\n",
    "        for row in rows:\n",
    "            cells = row.find_all('td')\n",
    "            if len(cells) >= 7:  # Ensure we have all required cells\n",
    "                match_data = {\n",
    "                    'team1': cells[0].get_text(strip=True),\n",
    "                    'team2': cells[1].get_text(strip=True),\n",
    "                    'winner': cells[2].get_text(strip=True),\n",
    "                    'margin': cells[3].get_text(strip=True),\n",
    "                    'ground': cells[4].get_text(strip=True),\n",
    "                    'matchDate': cells[5].get_text(strip=True),\n",
    "                    'scorecard': cells[6].get_text(strip=True)\n",
    "                }\n",
    "                match_summary.append(match_data)\n",
    "        \n",
    "        return {\n",
    "            \"matchSummary\": match_summary\n",
    "        }\n",
    "\n",
    "    def to_dataframe(self, data):\n",
    "        \"\"\"Convert the match summary data to a pandas DataFrame\"\"\"\n",
    "        if data and 'matchSummary' in data:\n",
    "            return pd.DataFrame(data['matchSummary'])\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    def save_to_csv(self, data, filename='cricket_matches.csv'):\n",
    "        \"\"\"Save the match summary data to a CSV file\"\"\"\n",
    "        df = self.to_dataframe(data)\n",
    "        if not df.empty:\n",
    "            df.to_csv(filename, index=False)\n",
    "            print(f\"\\nData has been saved to '{filename}'\")\n",
    "        else:\n",
    "            print(\"No data to save\")\n",
    "\n",
    "def main():\n",
    "    # Create scraper instance\n",
    "    scraper = CricketScraper()\n",
    "    \n",
    "    # Scrape the data\n",
    "    result = scraper.scrape_cricket_stats()\n",
    "    \n",
    "    if result:\n",
    "        # Convert to DataFrame and display\n",
    "        df = scraper.to_dataframe(result)\n",
    "        print(\"Successfully scraped match data:\")\n",
    "        print(df)\n",
    "        \n",
    "        # Save to CSV\n",
    "        scraper.save_to_csv(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd5862ef-5ef0-4291-a822-9f2920b99910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped match data:\n",
      "           team1         team2        winner      margin     ground  \\\n",
      "0        England      Pakistan       England   5 wickets  Melbourne   \n",
      "1        England         India       England  10 wickets   Adelaide   \n",
      "2    New Zealand      Pakistan      Pakistan   7 wickets     Sydney   \n",
      "3          India      Zimbabwe         India     71 runs  Melbourne   \n",
      "4     Bangladesh      Pakistan      Pakistan   5 wickets   Adelaide   \n",
      "5    Netherlands  South Africa   Netherlands     13 runs   Adelaide   \n",
      "6         Team 1        Team 2        Winner      Margin     Ground   \n",
      "7        England     Sri Lanka       England   4 wickets     Sydney   \n",
      "8      Australia   Afghanistan     Australia      4 runs   Adelaide   \n",
      "9        Ireland   New Zealand   New Zealand     35 runs   Adelaide   \n",
      "10      Pakistan  South Africa      Pakistan     33 runs     Sydney   \n",
      "11    Bangladesh         India         India      5 runs   Adelaide   \n",
      "12   Netherlands      Zimbabwe   Netherlands   5 wickets   Adelaide   \n",
      "13       England   New Zealand       England     20 runs   Brisbane   \n",
      "14   Afghanistan     Sri Lanka     Sri Lanka   6 wickets   Brisbane   \n",
      "15     Australia       Ireland     Australia     42 runs   Brisbane   \n",
      "16         India  South Africa  South Africa   5 wickets      Perth   \n",
      "17   Netherlands      Pakistan      Pakistan   6 wickets      Perth   \n",
      "18    Bangladesh      Zimbabwe    Bangladesh      3 runs   Brisbane   \n",
      "19   New Zealand     Sri Lanka   New Zealand     65 runs     Sydney   \n",
      "20      Pakistan      Zimbabwe      Zimbabwe       1 run      Perth   \n",
      "21         India   Netherlands         India     56 runs     Sydney   \n",
      "22    Bangladesh  South Africa  South Africa    104 runs     Sydney   \n",
      "23       England       Ireland       Ireland      5 runs  Melbourne   \n",
      "24     Australia     Sri Lanka     Australia   7 wickets      Perth   \n",
      "25  South Africa      Zimbabwe     no result           -     Hobart   \n",
      "26    Bangladesh   Netherlands    Bangladesh      9 runs     Hobart   \n",
      "27         India      Pakistan         India   4 wickets  Melbourne   \n",
      "28       Ireland     Sri Lanka     Sri Lanka   9 wickets     Hobart   \n",
      "29   Afghanistan       England       England   5 wickets      Perth   \n",
      "30     Australia   New Zealand   New Zealand     89 runs     Sydney   \n",
      "31      Scotland      Zimbabwe      Zimbabwe   5 wickets     Hobart   \n",
      "32       Ireland   West Indies       Ireland   9 wickets     Hobart   \n",
      "33       Namibia        U.A.E.        U.A.E.      7 runs    Geelong   \n",
      "34   Netherlands     Sri Lanka     Sri Lanka     16 runs    Geelong   \n",
      "35   West Indies      Zimbabwe   West Indies     31 runs     Hobart   \n",
      "36       Ireland      Scotland       Ireland   6 wickets     Hobart   \n",
      "37     Sri Lanka        U.A.E.     Sri Lanka     79 runs    Geelong   \n",
      "38       Namibia   Netherlands   Netherlands   5 wickets    Geelong   \n",
      "39       Ireland      Zimbabwe      Zimbabwe     31 runs     Hobart   \n",
      "40      Scotland   West Indies      Scotland     42 runs     Hobart   \n",
      "41   Netherlands        U.A.E.   Netherlands   3 wickets    Geelong   \n",
      "42       Namibia     Sri Lanka       Namibia     55 runs    Geelong   \n",
      "\n",
      "       matchDate    scorecard  \n",
      "0   Nov 13, 2022  T20I # 1879  \n",
      "1   Nov 10, 2022  T20I # 1878  \n",
      "2    Nov 9, 2022  T20I # 1877  \n",
      "3    Nov 6, 2022  T20I # 1873  \n",
      "4    Nov 6, 2022  T20I # 1872  \n",
      "5    Nov 6, 2022  T20I # 1871  \n",
      "6     Match Date    Scorecard  \n",
      "7    Nov 5, 2022  T20I # 1867  \n",
      "8    Nov 4, 2022  T20I # 1864  \n",
      "9    Nov 4, 2022  T20I # 1862  \n",
      "10   Nov 3, 2022  T20I # 1861  \n",
      "11   Nov 2, 2022  T20I # 1860  \n",
      "12   Nov 2, 2022  T20I # 1859  \n",
      "13   Nov 1, 2022  T20I # 1858  \n",
      "14   Nov 1, 2022  T20I # 1856  \n",
      "15  Oct 31, 2022  T20I # 1855  \n",
      "16  Oct 30, 2022  T20I # 1853  \n",
      "17  Oct 30, 2022  T20I # 1852  \n",
      "18  Oct 30, 2022  T20I # 1851  \n",
      "19  Oct 29, 2022  T20I # 1850  \n",
      "20  Oct 27, 2022  T20I # 1849  \n",
      "21  Oct 27, 2022  T20I # 1848  \n",
      "22  Oct 27, 2022  T20I # 1847  \n",
      "23  Oct 26, 2022  T20I # 1846  \n",
      "24  Oct 25, 2022  T20I # 1845  \n",
      "25  Oct 24, 2022  T20I # 1844  \n",
      "26  Oct 24, 2022  T20I # 1843  \n",
      "27  Oct 23, 2022  T20I # 1842  \n",
      "28  Oct 23, 2022  T20I # 1841  \n",
      "29  Oct 22, 2022  T20I # 1840  \n",
      "30  Oct 22, 2022  T20I # 1839  \n",
      "31  Oct 21, 2022  T20I # 1838  \n",
      "32  Oct 21, 2022  T20I # 1837  \n",
      "33  Oct 20, 2022  T20I # 1836  \n",
      "34  Oct 20, 2022  T20I # 1835  \n",
      "35  Oct 19, 2022  T20I # 1834  \n",
      "36  Oct 19, 2022  T20I # 1833  \n",
      "37  Oct 18, 2022  T20I # 1832  \n",
      "38  Oct 18, 2022  T20I # 1830  \n",
      "39  Oct 17, 2022  T20I # 1828  \n",
      "40  Oct 17, 2022  T20I # 1826  \n",
      "41  Oct 16, 2022  T20I # 1825  \n",
      "42  Oct 16, 2022  T20I # 1823  \n",
      "\n",
      "Data has been saved to 'cricket_matches.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "class CricketScraper:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://www.espncricinfo.com\"\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2.1 Safari/605.1.15'\n",
    "        }\n",
    "    \n",
    "    def scrape_cricket_stats(self, tournament_id=\"14450\"):\n",
    "        url = f'https://stats.espncricinfo.com/ci/engine/records/team/match_results.html?id={tournament_id};type=tournament'\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching the webpage: {e}\")\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        table = soup.find('table', class_='ds-table')\n",
    "        if not table:\n",
    "            print(\"Could not find the target table\")\n",
    "            return None\n",
    "        \n",
    "        match_summary = []\n",
    "        \n",
    "        # First get rows with ds-bg-ui-fill-translucent class (first 3 rows)\n",
    "        filled_rows = table.find_all('tr', class_='ds-bg-ui-fill-translucent')\n",
    "        # Then get rows with empty class\n",
    "        empty_rows = table.find_all('tr', class_='')\n",
    "        \n",
    "        # Combine all rows\n",
    "        all_rows = filled_rows + empty_rows\n",
    "        \n",
    "        for row in all_rows:\n",
    "            # Find all td elements with ds-min-w-max class\n",
    "            cells = row.find_all('td', class_='ds-min-w-max')\n",
    "            if len(cells) >= 7:\n",
    "                match_data = {\n",
    "                    'team1': cells[0].get_text(strip=True),\n",
    "                    'team2': cells[1].get_text(strip=True),\n",
    "                    'winner': cells[2].get_text(strip=True),\n",
    "                    'margin': cells[3].get_text(strip=True),\n",
    "                    'ground': cells[4].get_text(strip=True),\n",
    "                    'matchDate': cells[5].get_text(strip=True),\n",
    "                    'scorecard': cells[6].get_text(strip=True)\n",
    "                }\n",
    "                match_summary.append(match_data)\n",
    "        \n",
    "        return {\n",
    "            \"matchSummary\": match_summary\n",
    "        }\n",
    "\n",
    "    def to_dataframe(self, data):\n",
    "        if data and 'matchSummary' in data:\n",
    "            return pd.DataFrame(data['matchSummary'])\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    def save_to_csv(self, data, filename='cricket_matches.csv'):\n",
    "        df = self.to_dataframe(data)\n",
    "        if not df.empty:\n",
    "            df.to_csv(filename, index=False)\n",
    "            print(f\"\\nData has been saved to '{filename}'\")\n",
    "        else:\n",
    "            print(\"No data to save\")\n",
    "\n",
    "def main():\n",
    "    scraper = CricketScraper()\n",
    "    result = scraper.scrape_cricket_stats()\n",
    "    \n",
    "    if result:\n",
    "        df = scraper.to_dataframe(result)\n",
    "        print(\"Successfully scraped match data:\")\n",
    "        print(df)\n",
    "        scraper.save_to_csv(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b0de15-884c-4bae-b4dc-6956fa4fde5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
